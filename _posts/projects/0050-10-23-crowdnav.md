---
layout: project
permalink: /:title/
category: projects
# published: false

meta:
  keywords: "ROS"

project:
  title: "Quadruped Crowd Navigation"
  type: "Jekyll"
  url: "https://github.com/katie-hughes/unitree_crowd_nav"
  logo: "/assets/images/projects/crowdnav/dog-at-feet.jpg"
  tech: "ROS 2, C++, Optimal Control"

twomovingpedsoffboard: B1oihg262rQ
twostaticpedsoffboard: rSiHWG69YVA
onboard: BboH0EfObbY
fouronboard: aoRcOXswA70
orinnano: kkvdkUThQ_E
five_static_orin: ITsKZ7eEZUY
four_orin: HHSMcfS-1u4

---

For my capstone project for my MS in Robotics degree at Northwestern University, I am implementing the Bayes Rule Nash Equilibrium (BRNE) algorithm onto a Unitree Go1 robot for efficient crowd navigation. This post is not complete as I am still actively working on this project!

<br>

{% include youtubePlayer.html id=page.four_orin %}

<!-- <br> -->

<!-- {% include youtubePlayer.html id=page.onboard %} -->

<br>

# Algorithm

The BRNE algorithm was developed by Muchen Sun, my advisor for this project. The core idea of this algorithm is to model the strategy of each agent in the distribution space rather than a discrete path. A simple Bayesian update scheme to the distributions of each agent is guaranteed to converge to a global <a href="https://en.wikipedia.org/wiki/Nash_equilibrium" target="_blank"><u>Nash equilibrium</u></a> -- where each agent has an optimal path, and no strategy that any of the agents can take can improve their own trajectory.

<br>

The BRNE algorithm produces trajectories that minimize a cost function of the following form:

$$a_3 \cdot \left(2 - \frac{2}{1 + \exp\left(-a_1 \cdot d^{a_2}\right)} \right)$$

Where $$d$$ is the distance between two agents, and $$a_1$$, $$a_2$$, and $$a_3$$ are parameters that adjust how close the agents are willing to come to each other during their trajectory. This function can be thought of as the "risk" of this trajectory point.


![Description](/assets/images/projects/crowdnav/costs.gif)
<center><h6>Visualization of the cost function for the BRNE algorithm.</h6></center>

<br>

The first step in this algorithm is to generate trajectory samples for all agents. After this, the maximum cost for all trajectory samples between all pairs of agents is computed, which quantifies how "risky" each of these trajectories are. Finally, there is an iterative Bayesian update process which assigns a weight to every agents' possible trajectories. The optimal trajectory for each agent is the weighted sum of these samples. A simple example with two agents is outlined below. 

<br>

![Description](/assets/images/projects/crowdnav/BRNE-visualization.png)
<center><h6>In this example, two agents are in a hallway. The blue agent wants to walk to the right and the orange agent wants to walk to the left, and their nominal desired paths if no other agents were present are described by the dotted line. The left panel shows the randomly generated trajectory samples for each agent, and the right panel shows the optimal trajectories after 10 Bayesian updates. This also includes a final collision check to ensure that the samples that go out of the hallway's bounds are not considered. </h6></center>

<br>

My contribution to this algorithm has involved converting existing code from Python to C++ and optimizing certain matrix calculations. The Python version of this algorithm uses Numba for optimization. This has successfully worked in field tests, but requires around 20 seconds every time running to compile. Additionally, through stress tests, I found that the runtime blows up when the algorithm has to consider more than 4 agents (including the robot). 

<br>

My C++ implementation of the algorithm uses the Armadillo library for the matrix operations of the BRNE algorithm as well as OpenMP for parallelization. This is comparable with the Numba version of the algorithm for small numbers of pedestrians, but does not see the massive spike that the Python implementation does after 4 agents. In practice, the algorithm should provide trajectory updates at around 10 Hz and should be able to handle up to 5 agents. The C++ algorithm is better suited for this purpose and also scales more efficiently if more agents need to be considered in the future. 

![Description](/assets/images/projects/crowdnav/brne_speed_errorbar_cutoff8.png)
<center><h6>These times were generated running the appropriate BRNE ROS node in simulation using 196 samples per agent and 25 timesteps per sampled trajectory. The scatterplot points are the average computation time and the errorbars are the standard deviation over 100 timer iterations. </h6></center>


# System Design

At its core, the BRNE algorithm requires pedestrian location data and odometry data. Then, when given a goal position, it can iteratively create a trajectory plan that avoids pedestrians. These trajectory updates should be published every 10 Hz, and to ensure the algorithm is operating with the most up to date information, the perception data needs to be published faster than this baseline. The biggest challenge with the system design of this project was figuring out the best way to obtain the necessary perception information, as well as how to spread out the computation so that all processes can update at the appropriate rates.

![Description](/assets/images/projects/crowdnav/SystemDesignBase2.png)
<center><h6>Base Level System Design. I am using the control node in the 
<a href="https://github.com/ngmor/unitree_nav" target="_blank"><u>unitree_nav package</u></a> to process the commanded robot velocity from the trajectory plan into high level Unitree SDK commands. </h6></center>

## Iteration 1: All Unitree Hardware
The first plan for the system design was to use the built in USB cameras to the Unitree as well as the RS-Helios-16P LiDAR module it came with to provide the perception data. The Unitree Go1 internally has two Jetson Nano boards (one in the head and one in the body) and one Jetson Xavier board (in the body). The Jetson Nano boards are connected to USB cameras that are in the head and the right and left of the body. The idea was to have the Jetson Nano boards running YOLO-based people detection models which would feed into the BRNE algorithm. If this could be running on both Jetson Nano boards, you could track people from the front as well as the side of the robot, which is a big advantage over using a single camera. The odometry would come from using <a href="https://github.com/ngmor/unitree_nav" target="_blank"><u>the unitree_nav package</u></a> on the onbard Jetson Xavier which uses `rtabmap` and ICP odometry. 

![Description](/assets/images/projects/crowdnav/LidarDesign2.png)
<center><h6>Proposed system design with onboard LiDAR and onboard Unitree USB cameras.</h6></center>

There were a number of problems with this system. First, interacting with the Unitree cameras was very difficult. The cameras are unreadable by default, unlocked when you use the closed-source Unitree Camera SDK, and locked again when you stop using it. The Unitree camera SDK is also very buggy and and has dependencies on very old versions of OpenCV, which makes it difficult to compile on a 22.04 system for use with ROS 2. For around three weeks, I tried to find a workaround that would allow me to read from their cameras using standard Linux tools -- check out <a href="https://katie-hughes.github.io/cameras/" target="_blank"><u>this post</u></a> for more information on my reverse engineering progress -- but ultimately found it too much of a time sink. 

<br>

The main reason this system was not feasible was because the onboard Jetson boards on the Unitree were too slow. After setting up the LiDAR and running the nav2 stack on the Jetson Xavier, I only received odometry updates at 5 Hz. This was using a predefined map, so the nodes were only responsible for the computation of localizing. Mapping and localizing at the same time was not possible as if the robot moved too quickly, the map would not generate correctly. The need to have a map ahead of time also severely limited the deployability of this robot. Ultimately, I did not even attempt to deploy a people detection model on the Jetson Nanos, since if the more powerful Jetson Xavier could not compute relatively simple odometry updates quickly enough, there was no chance that a more complicated ML model on the Jetson Nano would be able to keep up with the speed requirements. 
<br>

## Iteration 2: Laptop Mount + ZED camera
As pedestrian tracking and odometry updates from the onboard Unitree devices was not feasible, we decided to handle perception using a ZED 2i camera mounted onto the back of the robot. The ZED camera provides <a href="https://www.stereolabs.com/docs/positional-tracking/" target="_blank"><u>robust visual odometry</u></a> and has built in ML models that are capable of <a href="https://www.stereolabs.com/docs/object-detection/" target="_blank"><u>3D object tracking</u></a>. The constraint of using the ZED is that it requires CUDA and a powerful GPU, which none of the onboard Jetson boards were able to provide. For this purpose, we mounted a System76 Adder WS laptop to the back of the Unitree so that it could process the perception information. Since the BRNE algorithm is CPU intensive while ZED processing is GPU intensive, it is possible to run essentially the entire system on this mounted laptop. 

![Description](/assets/images/projects/crowdnav/LaptopMountDesign.png)
<center><h6>System design with an onboard laptop (thanks to Davin Landry for designing and printing the mount!)</h6></center>

While this performs all of the relevant computation onboard, there are some downsides to this approach. First, it was difficult to design a mount that securely held the laptop on the back of the robot. The robot also has trouble with agile movements and is more prone to falling over from imbalance. With all of the extra weight, the motors also overheat faster, and devices onboard the robot tend to fail sooner (namely the Raspberry Pi provided WIFI hotspot).


## Iteration 3 (and current design): Orin Nano + ZED camera
The current system uses a Jetson Orin Nano board mounted to the Unitree and connected to the ZED camera to handle the perception updates. We initially did not expect the Orin Nano to be able to handle this level of computation, but it was able to provide pedestrian locations and robot odometry at 15 Hz, which is fast enough for the BRNE algorithm. This system is much lighter than the mounted laptop and makes the robot much more agile. The core BRNE algorithm executes on my external computer as the Orin Nano does not have good enough CPU for the trajectory optimization calculations. In the future, it might be possible to use CUDA to optimize the performance of the algorithm to this device, and once again have everything onboard. 

![Description](/assets/images/projects/crowdnav/OrinNanoDesign2.png)
<center><h6>System design with an onboard Orin Nano (thanks to David Dorf for designing and printing the mount!)</h6></center>

The BRNE algorithm needs the position and velocity of nearby pedestrians. While the ZED camera is very robust at tracking pedestrian position, its velocity estimate is not very consistent. We found it was a better estimate to do a simple frame-to-frame velocity calculation by comparing the location of each pedestrian in subsequent frames and dividing by the time difference between the updates. This is also run on the Orin Nano and results are shown below. While this approach is still not perfect, it is orders of magnitude smoother than using the raw message data.

<br>

![Description](/assets/images/projects/crowdnav/f2f.gif)
<center><h6>Result of pedestrian position tracking from the ZED camera and my custom frame-to-frame velocity estimation. </h6></center>

# Acknowledgments

*  <a href="https://muchen-sun.com/" target="_blank"><u>Muchen Sun</u></a>
*  <a href="https://murpheylab.github.io/" target="_blank"><u>Todd Murphey</u></a>
*  <a href="https://robotics.northwestern.edu/people/profiles/faculty/elwin-matt.html" target="_blank"><u>Matthew Elwin</u></a>
*  <a href="https://dlandry97.github.io/Davin_Landry/" target="_blank"><u>Davin Landry</u></a>
*  <a href="https://www.daviddorf.com/" target="_blank"><u>David Dorf</u></a>


<br>



<details>
  <summary><b>Video Archive</b></summary>
  <br>
  {% include youtubePlayer.html id=page.five_static_orin %}
  <br>
  {% include youtubePlayer.html id=page.orinnano %}
  <br>
  {% include youtubePlayer.html id=page.fouronboard %}
  <br>
  {% include youtubePlayer.html id=page.twostaticpedsoffboard %}
  <br>
  {% include youtubePlayer.html id=page.twomovingpedsoffboard %}
  <br>
  {% include youtubePlayer.html id=page.onboard %}
</details>



<br><br>

