---
layout: project
permalink: /:title/
category: projects

meta:
  keywords: "ROS"

project:
  title: "Robot Jenga Assistant"
  type: "Jekyll"
  url: "https://github.com/ME495-EmbeddedSystems/hw3group-jengabells"
  logo: "/assets/images/projects/jenga/pulling.png"
  tech: "ROS2, Computer Vision, MoveIt2, Python"

7pieces: ST-r9X2dZMg
10pieces: ZFCv9j5TN58
---



<p>For my final project for ME495: Embedded Systems in Robotics, my group and I created a ROS2 package to turn an Franka Emika Panda 7DOF arm into a Jenga assistant.</p> 

<br>

{% include youtubePlayer.html id=page.10pieces %}

<br>
<p>Our core goal was to have the robot play alongside a human. The user would select a block and partially remove it from the tower. The robot would then detect this block, remove it the rest of the way, and place it on the top of the tower.</p>
<br>

<p>My main role on this project centered around computer vision -- given a camera image, how do we detect where the Jenga tower and the partially removed pieces are? This goal was made easier by using a camera that has a depth frame (an Intel Realsense D435i) and choosing a clever camera placement of directly above the center of the tower, looking down. 
From this viewpoint, you will see roughly three groups of objects that exist at distinct distances from the camera. The first is the top of the tower, which is closest to the camera, has a fairly large area, and should be square-shaped at the beginning of the game. The second is the table, which is farthest from the camera and also has a very large contour area. The third is any pieces that might be partially sticking out from the side of the tower. These pieces will always exist at a distance that is somewhere between the top of the tower and the table.
Upon startup, the CV node performs a scan through the depth frame to locate the top of the tower and the table. Then, it repeatedly scans between these two locations to search for pieces to pull. When one is found, the CV node broadcasts the location of its centroid with respect to the camera, so that the movement node can start planning to grab the piece. </p>

<br>

![Description](/assets/images/projects/jenga/scanning_cropped.gif)
<center><h2>Early work scanning to detect the top of the tower, a piece, and the table within a square region surrounding the tower. </h2></center>


<!-- <br>

![Description](/assets/images/projects/jenga/contours2.png)
<center><h2>Detecting a piece using the depth frame information. The large blue box desginates the border, anything outside of that boundary is disregarded. The green outlines are contours of large area at this particular "slice" of the depth frame. I choose the largest contour as the piece. The red point is the centroid of this largest contour, and where the arm will grab. </h2></center> -->

<br>

The vision node also computes the orientation of the top of the tower on its initial scan. This is necessary so that the pieces can be placed in the  For this, it applies Canny edge detection and Hough lines on the portion of the color image that is aligned with the object in the depth frame. 

<br>

![Description](/assets/images/projects/jenga/edges.png)
<center><h2>Detecting the orientation of the tower top</h2></center>

<br>

I also briefly worked with the movement nodes. A given "turn" is defined by the starting location of the block and the ending location to place it on the top of the tower. The starting location is computed during the CV scans. The ending locations are precomputed based on the initial height/orientation of the tower and the width of the pieces (as you need to stack 3 pieces adjacent to each other). We followed a 
<br><br>

![Description](/assets/images/projects/jenga/destroy_2x.gif)
<center><h2>Frank destroying the tower in a fit of rage!<br>(2x speed)</h2></center>

<br>

<!-- {% include youtubePlayer.html id=page.10pieces %} -->

